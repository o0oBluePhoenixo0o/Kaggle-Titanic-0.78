# No need to do this since tokens will be put behind "$" 01.05.2018
# # Load token list --> get exceptions for not being mistaken as acronyms
# coins_list <- read_csv("./1. Crawlers/Crypto-Markets_2018-04-30.csv")
#
# tokens <- tolower(as.vector(unique(coins_list$symbol)))
# test <- as.data.frame(setdiff(myAbbrevs$abv,tokens))
# names(test) <- 'abv'
# # Keep only non-token abbreviations
# myAbbrevs <- dplyr::left_join(test,myAbbrevs, by = 'abv')
# rm(test)
# Convert dataframe to dictionary list
t_myAbbrevs <- t(myAbbrevs$rep)
names(t_myAbbrevs) <- myAbbrevs$abv
convertAbbreviations <- function(message){
# Replaces abbreviation with the corresporending long form
#
# Args:
#   text: Text to remove the abbreviations from
#
# Returns:
#   String
if(is.na(message) || message == ""){
return(message)
} else {
message_split <- strsplit(message,"\\s")
for (i in 1:lengths(message_split)){
try(message_split[[1]][i] <- t_myAbbrevs[[message_split[[1]][i]]],
silent = TRUE)
}
# Remerge list into string
newText <- paste(unlist(message_split), collapse=' ')
return (newText)
}
}
# Tag POS 14.05
tagPOS_lemma <-  function(x, ...) {
# POS original string
s <- as.String(x)
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- NLP::annotate(s, word_token_annotator, a2)
a3 <- NLP::annotate(s, Maxent_POS_Tag_Annotator(), a2)
a3w <- a3[a3$type == "word"]
# Collect POS tagging
POStags <- unlist(lapply(a3w$features, `[[`, "POS"))
POStagged <- paste(sprintf("%s_%s", s[a3w], POStags), collapse = " ")
gc() # garbage collection
# Lemmatization then remerge with POS-tag
x <- unlist(strsplit(POStagged," "))
for (i in 1:length(x[[1]])){
txt <- gsub("_[^_]+$", "", x[[1]][i]) # capture everything before "_"
x[[1]][i] <- gsub("^[^_]+", "", x[[1]][1]) #replace the captured part with blank
txt <- textstem::lemmatize_words(txt)
# Re-add lemmatized string back to POS-tag
x[[1]][i] <- paste0(txt,x[[1]][i])
}
result <- paste(x,collapse = " ")
return(result)
}
###################################################################
Cleandata <- function(df) {
# Cross-validating with list of Twitter_bots
bots <- openxlsx::read.xlsx('~/GitHub/NextBigCrypto-Senti/0. Datasets/Twitter_Bot_Users_(Final).xlsx')
df <- inner_join(df,bots, by = 'screen_name')
df <- df %>% filter(botprob < 0.85| is.na(botprob)) # filter users that are >85% chance a bot
# Convert unicode
df$text <- sapply(df$text,function(x) trueunicode.hack(x))
# remove duplicates base on tweets
df <- df[!duplicated(df$text),]
df$processed <- sapply(df$text, function(x) removeURL(x)) # remove URL
# To lower case
df$processed <- sapply(df$processed, function(x) tolower(x))
df$processed <- sapply(df$processed, function(x) gsub("[.,]"," ", x, perl = TRUE)) #remove . and ,
# Remove duplicates
df <- df[!duplicated(df$processed),]
# converting abbreviations
df$processed <- sapply(df$processed, function(x) convertAbbreviations(x))
df$processed <- sapply(df$processed, function(x) conv_fun(x)) # convert to delete emojis
df$processed <- sapply(df$processed, function(x) gsub("[\r\n]", " ", x)) #change /r /n break lines into space
# remove stopwords - create exception lists 25.04
exceptions   <- c('up','down','all','above','below','under','over',
'few','more', 'in')
# keep negation list
negations <- grep(pattern = "not|n't", x = stopwords(), value = TRUE)
exceptions <- c(exceptions,negations)
my_stopwords <- setdiff(stopwords("en"), exceptions)
df$processed <- sapply(df$processed, function(x) removeWords(x,c(my_stopwords)))
###########################################
# Get rid of references to other screennames
df$processed <- str_replace_all(df$processed,"@[a-z,A-Z,_]*"," ")
# remove punctuations except for # $
df$processed <- sapply(df$processed, function(x) gsub( "[^#$a-zA-Z\\s]" , "" , x , perl = TRUE ))
# Apply Apos to space
df$processed <- sapply(df$processed, function(x) AposToSpace(x))
# removing number 02.03.18
df$processed <- sapply(df$processed, function(x) removeNumbers(x))
# Remove left-overs
df$processed <- sapply(df$processed, function(x) gsub("ff", " ",x))
df$processed <- sapply(df$processed, function(x) gsub("# ", " ", x))
df$processed <- sapply(df$processed, function(x) gsub(" f ", " ", x))
# remove whitespace before & after
df$processed <- sapply(df$processed, function(x) gsub("^[[:space:]]+", "",x))
df$processed  <- sapply(df$processed, function(x) gsub("[[:space:]]+$", "",x))
df$processed <- sapply(df$processed, function(x) stripWhitespace(x))
# Remove blank processed messages
df <- df[!(is.na(df$processed) | df$processed %in% c(""," ")), ]
# Remove duplicates
df <- df[!duplicated(df$processed),]
# Lemmatization 26.04.18
df$processed <- sapply(df$processed, function(x) textstem::lemmatize_strings(x))
return(df)
}
options(stringsAsFactors = FALSE)
comments.df <- readr::read_csv('~/GitHub/NextBigCrypto-Senti/1. Crawlers/2b. Reddit Report/Crypto_Reddit_comments.csv')
Cleandata <- function(df) {
# remove duplicates base on contents
df <- df[!duplicated(df$com_content),]
df$processed <- sapply(df$com_content, function(x) removeURL(x)) # remove URL
# To lower case
df$processed <- sapply(df$processed, function(x) tolower(x))
df$processed <- sapply(df$processed, function(x) gsub("[.,]"," ", x, perl = TRUE)) #remove . and ,
# Remove duplicates
df <- df[!duplicated(df$processed),]
# converting abbreviations
df$processed <- sapply(df$processed, function(x) convertAbbreviations(x))
df$processed <- sapply(df$processed, function(x) conv_fun(x)) # convert to delete emojis
df$processed <- sapply(df$processed, function(x) gsub("[\r\n]", " ", x)) #change /r /n break lines into space
# remove stopwords - create exception lists 25.04
exceptions   <- c('up','down','all','above','below','under','over',
'few','more', 'in')
# keep negation list
negations <- grep(pattern = "not|n't", x = stopwords(), value = TRUE)
exceptions <- c(exceptions,negations)
my_stopwords <- setdiff(stopwords("en"), exceptions)
df$processed <- sapply(df$processed, function(x) removeWords(x,c(my_stopwords)))
###########################################
# Get rid of references to other screennames
df$processed <- str_replace_all(df$processed,"@[a-z,A-Z,_]*"," ")
# remove punctuations
df$processed <- sapply(df$processed, function(x) gsub( "[^a-zA-Z\\s]" , "" , x , perl = TRUE ))
# Apply Apos to space
df$processed <- sapply(df$processed, function(x) AposToSpace(x))
# removing number 02.03.18
df$processed <- sapply(df$processed, function(x) removeNumbers(x))
# Remove left-overs
df$processed <- sapply(df$processed, function(x) gsub("ff", " ",x))
df$processed <- sapply(df$processed, function(x) gsub("# ", " ", x))
df$processed <- sapply(df$processed, function(x) gsub(" f ", " ", x))
# remove whitespace before & after
df$processed <- sapply(df$processed, function(x) gsub("^[[:space:]]+", "",x))
df$processed  <- sapply(df$processed, function(x) gsub("[[:space:]]+$", "",x))
df$processed <- sapply(df$processed, function(x) stripWhitespace(x))
# Remove blank processed messages
df <- df[!(is.na(df$processed) | df$processed %in% c(""," ")), ]
# Remove duplicates
df <- df[!duplicated(df$processed),]
# Lemmatization 26.04.18
df$processed <- sapply(df$processed, function(x) textstem::lemmatize_strings(x))
return(df)
}
comments.raw <- split(comments.df, (seq(nrow(comments.df))-1) %/% 50000)
#comments.clean <- Cleandata(comments.df)
as.data.frame(comments.raw[['0']])
final.clean <- comments.df[0,]
for (i in 1:length(comments.raw)){
df <- as.data.frame(comments.raw[[i]])
clean.df <- Cleandata(df)
final.clean <- bind_rows(final.clean,clean.df)
print(paste0('Complete batch ',i,'/',length(comments.raw)))
}
for (i in 1:length(comments.raw)){
df <- as.data.frame(comments.raw[[i]])
clean.df <- Cleandata(df)
final.clean <- bind_rows(final.clean,clean.df)
print(paste0('Complete batch ',i,'/',length(comments.raw)))
write_csv(final.clean,'~/GitHub/NextBigCrypto-Senti/0. Datasets/Reddit.cleanBK.csv')
}
View(final.clean)
View(comments.raw)
not.done <- anti_join(comments.df,final.clean, by = 'post_id')
clean.not.done <- Cleandata(not.done)
final.clean <- bind_rows(final.clean,clean.not.done)
write_csv(final.clean,'~/GitHub/NextBigCrypto-Senti/0. Datasets/Reddit.cleanBK.csv')
# Test
a <- 100
b <- 100
if (a = 100 & b = 100){print('Yes')}
if (a == 100 & b == 100){print('Yes')}
load('~\GitHub\NextBigCrypto-Senti\Models\BCH_LDA.RData')
load('~/GitHub/NextBigCrypto-Senti/Models/BCH_LDA.RData')
library(ldatuning)
ldatuning::FindTopicsNumber_plot(result)
token_name <- 'BCH'
compare.w.BTC <- 1
# Read in coin list as Oct 17
coins_list <- read.csv("~/GitHub/NextBigCrypto-Senti/1. Crawlers/Top50_Oct7.csv")
position <- match(token_name, coins_list$symbol) # get position in queue
# compare price in BTC / USD
if (compare.w.BTC == 1){compare.w.BTC <- '_wBTC'}
if (compare.w.BTC == 0){compare.w.BTC <- '_wUSD'}
files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
# files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/3. Models Development/Results/',
#                     pattern = paste0('^',position,'.',token_name,'_','*[:digit:]',
#                                      '_',compare.w.BTC,'_'))
# substr(files[1],14,nchar(files[1])-5)
# test <- strsplit(files[2],'_')[[1]][2] # extract the position
for (i in 1:length(files)){
test <- strsplit(files[i],'_')[[1]][2] # extract the position
if (nchar(test) == 1){name <- substr(files[i],14,nchar(files[i])-5)}
if (nchar(test) == 2){name <- substr(files[i],15,nchar(files[i])-5)}
results <- readxl::read_xlsx(paste0('~/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',files[i]))
# Final results file
if (i == 1){results.final <- cbind(name,results)}
if (i != 1){
results <- cbind(name, results)
results.final <- rbind(results.final, results)}
}
# Save final result
openxlsx::write.xlsx(results.final,paste0('~/GitHub/NextBigCrypto-Senti/4. Results/',
position,'.',token_name, compare.w.BTC,'_FINAL.xlsx'))
# clear the environment
rm(list= ls())
gc()
#################
#               #
# CONTROL PANEL #
#               #
#################
# Edit variables here for the model you want to generate
# crypto token to be built models on
token_name <- NA
# compare price with USD$ or BTC (Bitcoin) (0 = USD / 1 = BTC)
compare.w.BTC <- NA
# Legend
# - Historical Price (HP)
# - Sentiment Analysis (trained) (SAT)
# - Sentiment Analysis (packages) (SAP)
# - Pre-defined topics (PD)
# Change the flag here (1 = true / 0 = false)
# Example, model 4 would be SAT = 1 and HP = 1 while the rest are 0
model.list <- data.frame(HP = numeric(),
SAT = numeric(),
SAP = numeric(),
LDA = numeric(),
PD = numeric())
# 1.  HP
model.list[1,] <- c(1,0,0,0,0)
# 2.  SAT
model.list[2,] <- c(0,1,0,0,0)
# 3.  SAP
model.list[3,] <- c(0,0,1,0,0)
# 4.  SAT - HP
model.list[4,] <- c(1,1,0,0,0)
# 5.  SAP - HP
model.list[5,] <- c(1,0,1,0,0)
# 6.  LDA
model.list[6,] <- c(0,0,0,1,0)
# 7.  LDA - HP
model.list[7,] <- c(1,0,0,1,0)
# 8.  LDA - SAT
model.list[8,] <- c(0,1,0,1,0)
# 9.  LDA - SAT - HP
model.list[9,] <- c(1,1,0,1,0)
# 10. LDA - SAP
model.list[10,] <- c(0,0,1,1,0)
# 11. LDA - SAP - HP
model.list[11,] <- c(1,0,1,1,0)
# 12. PD
model.list[12,] <- c(0,0,0,0,1)
# 13. PD - HP
model.list[13,] <- c(1,0,0,0,1)
# 14. PD - SAT
model.list[14,] <- c(0,1,0,0,1)
# 15. PD - SAT - HP
model.list[15,] <- c(1,1,0,0,1)
# 16. PD - SAP
model.list[16,] <- c(0,0,1,0,1)
# 17. PD - SAP - HP
model.list[17,] <- c(1,0,1,0,1)
ID <- seq.int(nrow(model.list))
model.list <- cbind(ID,model.list)
# Function to get model name (for later use - combine all results together)
get.model.name <- function(model.list,model.no,compare.w.BTC,position){
title.final <- ''
if (model.list$HP == 1){title.final <- paste0(title.final,'_HP')}
if (model.list$SAT == 1){title.final <- paste0(title.final,'_SAT')}
if (model.list$SAP == 1){title.final <- paste0(title.final,'_SAP')}
if (model.list$LDA == 1){title.final <- paste0(title.final,'_LDA')}
if (model.list$PD == 1){title.final <- paste0(title.final,'_PD')}
# compare price in BTC / USD
if (compare.w.BTC == 1){compare.w.BTC <- '_wBTC'}
if (compare.w.BTC == 0){compare.w.BTC <- '_wUSD'}
result_filename <- paste0(position,'.',token_name,'_',model.no, compare.w.BTC, title.final)
return(result_filename)
}
# Read in coin list as Oct 17
coins_list <- read.csv("~/GitHub/NextBigCrypto-Senti/1. Crawlers/Top50_Oct7.csv")
token_name <- 'BCH'
compare.w.BTC <- 1
# Read in coin list as Oct 17
coins_list <- read.csv("~/GitHub/NextBigCrypto-Senti/1. Crawlers/Top50_Oct7.csv")
position <- match(token_name, coins_list$symbol) # get position in queue
# compare price in BTC / USD
if (compare.w.BTC == 1){compare.w.BTC <- '_wBTC'}
if (compare.w.BTC == 0){compare.w.BTC <- '_wUSD'}
files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
# files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/3. Models Development/Results/',
#                     pattern = paste0('^',position,'.',token_name,'_','*[:digit:]',
#                                      '_',compare.w.BTC,'_'))
# substr(files[1],14,nchar(files[1])-5)
# test <- strsplit(files[2],'_')[[1]][2] # extract the position
for (i in 1:length(files)){
test <- strsplit(files[i],'_')[[1]][2] # extract the position
if (nchar(test) == 1){name <- substr(files[i],14,nchar(files[i])-5)}
if (nchar(test) == 2){name <- substr(files[i],15,nchar(files[i])-5)}
results <- readxl::read_xlsx(paste0('~/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',files[i]))
# Final results file
if (i == 1){results.final <- cbind(name,results)}
if (i != 1){
results <- cbind(name, results)
results.final <- rbind(results.final, results)}
}
# Save final result
openxlsx::write.xlsx(results.final,paste0('~/GitHub/NextBigCrypto-Senti/4. Results/',
position,'.',token_name, compare.w.BTC,'_FINAL.xlsx'))
files
files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/4.Results/Results_separated/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
token_name <- 'BCH'
compare.w.BTC <- 1
# Read in coin list as Oct 17
coins_list <- read.csv("~/GitHub/NextBigCrypto-Senti/1. Crawlers/Top50_Oct7.csv")
position <- match(token_name, coins_list$symbol) # get position in queue
# compare price in BTC / USD
if (compare.w.BTC == 1){compare.w.BTC <- '_wBTC'}
if (compare.w.BTC == 0){compare.w.BTC <- '_wUSD'}
files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
# files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/3. Models Development/Results/',
#                     pattern = paste0('^',position,'.',token_name,'_','*[:digit:]',
#                                      '_',compare.w.BTC,'_'))
# substr(files[1],14,nchar(files[1])-5)
# test <- strsplit(files[2],'_')[[1]][2] # extract the position
for (i in 1:length(files)){
test <- strsplit(files[i],'_')[[1]][2] # extract the position
if (nchar(test) == 1){name <- substr(files[i],14,nchar(files[i])-5)}
if (nchar(test) == 2){name <- substr(files[i],15,nchar(files[i])-5)}
results <- readxl::read_xlsx(paste0('~/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',files[i]))
# Final results file
if (i == 1){results.final <- cbind(name,results)}
if (i != 1){
results <- cbind(name, results)
results.final <- rbind(results.final, results)}
}
# Save final result
openxlsx::write.xlsx(results.final,paste0('~/GitHub/NextBigCrypto-Senti/4. Results/',
position,'.',token_name, compare.w.BTC,'_FINAL.xlsx'))
files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
getwd
getwd()
files <- list.files(path = 'I:/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
setwd('I:/GitHub/NextBigCrypto-Senti/4. Results/Results_separated/')
files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/3. Models Development/Results/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/3. Models Development/Results/',
pattern = paste0('^',position,'.',token_name,'_','\\d*',compare.w.BTC,'_'))
# files <- list.files(path = '~/GitHub/NextBigCrypto-Senti/3. Models Development/Results/',
#                     pattern = paste0('^',position,'.',token_name,'_','*[:digit:]',
#                                      '_',compare.w.BTC,'_'))
# substr(files[1],14,nchar(files[1])-5)
# test <- strsplit(files[2],'_')[[1]][2] # extract the position
for (i in 1:length(files)){
test <- strsplit(files[i],'_')[[1]][2] # extract the position
if (nchar(test) == 1){name <- substr(files[i],14,nchar(files[i])-5)}
if (nchar(test) == 2){name <- substr(files[i],15,nchar(files[i])-5)}
results <- readxl::read_xlsx(paste0('~/GitHub/NextBigCrypto-Senti/3. Models Development/Results/',files[i]))
# Final results file
if (i == 1){results.final <- cbind(name,results)}
if (i != 1){
results <- cbind(name, results)
results.final <- rbind(results.final, results)}
}
# Save final result
openxlsx::write.xlsx(results.final,paste0('~/GitHub/NextBigCrypto-Senti/3. Models Development/',
position,'.',token_name, compare.w.BTC,'_FINAL.xlsx'))
setwd("~/GitHub/Kaggle")
setwd("~/GitHub/Kaggle/Titanic")
knitr::opts_chunk$set(echo = TRUE)
# set working directory
setwd("~/GitHub/Kaggle/Titanic")
# load packages and set options
options(stringsAsFactors = FALSE)
# install packages if not available
packages <- c("readr", #read data
"lubridate", #date time conversion
"dplyr", #data manipulation
"h2o", "caret", # ML libs
"Hmisc" # EDA
)
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# set working directory
setwd("~/GitHub/Kaggle/Titanic")
test <- read.csv("test.csv")
train <- read.csv("train.csv")
output_sample <- read.csv("gender_submission.csv")
View(output_sample)
View(test)
View(train)
describe(train)
test$Survive <- NA
test <- test %>% select(-Survive)
# Add target variable
test$Survived <- NA
col(test)
colnames(test)
full <- rbind(train,test)
View(full)
missing_values <- full %>% summarize_all(funs(sum(is.na(.))/n()))
missing_values <- gather(missing_values, key="feature", value="missing_pct")
??gather
?gather
# install packages if not available
packages <- c("readr", #read data
"lubridate", #date time conversion
"dplyr", "tidyr" #data manipulation
"h2o", "caret", # ML libs
# install packages if not available
packages <- c("readr", #read data
"lubridate", #date time conversion
"dplyr", "tidyr", #data manipulation
"h2o", "caret", # ML libs
"Hmisc" # EDA
)
lapply(packages, require, character.only = TRUE)
missing_values <- full %>% summarize_all(funs(sum(is.na(.))/n()))
missing_values <- gather(missing_values, key="feature", value="missing_pct")
missing_values %>%
ggplot(aes(x=reorder(feature,-missing_pct),y=missing_pct)) +
geom_bar(stat="identity",fill="red")+
coord_flip()+theme_bw()
missing_values %>%
ggplot(aes(x=reorder(feature,-missing_pct),y=missing_pct)) +
geom_bar(stat="identity",fill="blue")+
coord_flip()+theme_bw()
miss_pct <- map_dbl(full, function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })
?map_dbl
miss_pct <- purr::map_dbl(full, function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })
miss_pct <- purrr::map_dbl(full, function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })
miss_pct <- miss_pct[miss_pct > 0]
data.frame(miss=miss_pct, var=names(miss_pct), row.names=NULL) %>%
ggplot(aes(x=reorder(var, -miss), y=miss)) +
geom_bar(stat='identity', fill='red') +
labs(x='', y='% missing', title='Percent missing data by feature') +
theme(axis.text.x=element_text(angle=90, hjust=1))
# install packages if not available
packages <- c("readr", #read data
"lubridate", #date time conversion
"dplyr", "naniar", #data manipulation
"h2o", "caret", # ML libs
"Hmisc" # EDA
)
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
install.packages(setdiff(packages, rownames(installed.packages())))
install.packages(setdiff(packages, rownames(installed.packages())))
lapply(packages, require, character.only = TRUE)
#Check for Missing values
vis_miss(full)
# Get overview of training data
describe(full)
#Check for Missing values
vis_miss(full)
View(output_sample)
View(missing_values)
rm(missing_values)
glimpse(full)
#Check for Missing values
vis_dat(full)
#Check for Missing values
vis_dat::vis_dat(full)
# install packages if not available
packages <- c("readr", #read data
"lubridate", #date time conversion
"dplyr", "naniar","vis_dat", #data exploratory + manipulation
"h2o", "caret", # ML libs
"Hmisc" # EDA
)
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
install.packages(c("anytime", "bit", "Boruta", "broom", "C50", "callr", "car", "carData", "caret", "caTools", "chron", "cli", "crypto", "Cubist", "CVST", "data.table", "dbplyr", "ddalpha", "devtools", "digest", "dimRed", "doParallel", "dplyr", "DT", "e1071", "english", "evaluate", "forecast", "Formula", "gbm", "ggplot2", "ggrepel", "git2r", "glue", "gmp", "gtools", "h2o", "haven", "highr", "htmlTable", "htmlwidgets", "httpuv", "igraph", "ipred", "ISOcodes", "iterators", "kernlab", "koRpus", "labelled", "lambda.r", "later", "lava", "lexicon", "lme4", "magic", "maptools", "memisc", "mime", "miniUI", "ModelMetrics", "modelr", "modeltools", "munsell", "mvtnorm", "nloptr", "NLP", "openssl", "openxlsx", "padr", "party", "partykit", "pillar", "pkgconfig", "plotrix", "progress", "psych", "purrr", "qdap", "Quandl", "quanteda", "quantreg", "questionr", "R6", "ranger", "RApiDatetime", "rapidjsonr", "raster", "Rcpp", "RcppArmadillo", "RcppParallel", "RcppProgress", "RcppRoll", "RCurl", "recipes", "repr", "reprex", "reticulate", "rJava", "rjson", "rmarkdown", "Rmpfr", "robustbase", "RSpectra", "rstudioapi", "Rttf2pt1", "rtweet", "RWeka", "RWekajars", "sandwich", "scales", "sentimentr", "servr", "shiny", "slam", "smbinning", "snow", "sourcetools", "sp", "spacyr", "sparsepp", "spData", "statnet.common", "stringdist", "stringi", "stringr", "sylly", "testthat", "textcat", "textclean", "textfeatures", "textmineR", "textshape", "TH.data", "tidyquant", "tidyr", "tidyselect", "tidytext", "timetk", "tm", "tseries", "TTR", "utf8", "wordcloud", "xfun", "xlsx", "XML", "xtable", "xts", "yaml", "zoo"))
